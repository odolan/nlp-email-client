{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "Classifying with Naive Bayes...\n",
      "Classifying with GPT...\n",
      "\n",
      "Model Comparison:\n",
      "         Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Naive Bayes     0.905        1.0  0.797872  0.887574\n",
      "1          GPT     1.000        1.0  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from run_classifier import Classifier as nb_classifier\n",
    "from run_gpt_classifier import GPTClassifier as gpt_classifier\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# initalize classifiers\n",
    "gpt_classifier = gpt_classifier(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "nb_classifier = nb_classifier(\"naive_bayes_model.json\")\n",
    "\n",
    "# load test data\n",
    "test_data_path = \"../../data/test_emails.csv\" \n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# extract text and labels\n",
    "emails = test_data[\"text\"].values\n",
    "true_labels = test_data[\"label\"].values\n",
    "\n",
    "# lists to store predictions\n",
    "naive_bayes_predictions = []\n",
    "gpt_predictions = []\n",
    "\n",
    "# Naive Bayes classification\n",
    "print(\"Classifying with Naive Bayes...\")\n",
    "for email in emails:\n",
    "    prediction = nb_classifier.classify(email)\n",
    "    naive_bayes_predictions.append(1 if prediction == \"Spam\" else 0)\n",
    "\n",
    "# GPT classification\n",
    "print(\"Classifying with GPT...\")\n",
    "for email in emails:\n",
    "    prediction = gpt_classifier.classify(email)\n",
    "    gpt_predictions.append(1 if prediction == \"Spam\" else 0)\n",
    "\n",
    "# convert to numpy arrays for easier comparison\n",
    "naive_bayes_predictions = np.array(naive_bayes_predictions)\n",
    "gpt_predictions = np.array(gpt_predictions)\n",
    "\n",
    "# calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "# calculate precision\n",
    "def precision(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "# calculate recall\n",
    "def recall(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "# calculate F1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "# calculate metrics for Naive Bayes\n",
    "naive_bayes_accuracy = accuracy(true_labels, naive_bayes_predictions)\n",
    "naive_bayes_precision = precision(true_labels, naive_bayes_predictions)\n",
    "naive_bayes_recall = recall(true_labels, naive_bayes_predictions)\n",
    "naive_bayes_f1 = f1_score(true_labels, naive_bayes_predictions)\n",
    "\n",
    "# calculate metrics for GPT\n",
    "gpt_accuracy = accuracy(true_labels, gpt_predictions)\n",
    "gpt_precision = precision(true_labels, gpt_predictions)\n",
    "gpt_recall = recall(true_labels, gpt_predictions)\n",
    "gpt_f1 = f1_score(true_labels, gpt_predictions)\n",
    "\n",
    "# display results\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Naive Bayes\", \"GPT\"],\n",
    "    \"Accuracy\": [naive_bayes_accuracy, gpt_accuracy],\n",
    "    \"Precision\": [naive_bayes_precision, gpt_precision],\n",
    "    \"Recall\": [naive_bayes_recall, gpt_recall],\n",
    "    \"F1 Score\": [naive_bayes_f1, gpt_f1]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results)\n",
    "\n",
    "# save results to a CSV file\n",
    "results.to_csv(\"model_comparison_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
