{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 96956\n",
      "Validation size: 24239\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from JSON file, with each object separated by a newline\n",
    "file_path = \"data/email_summaries_data.json\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Ensure the dataset has 'email' and 'summary' columns\n",
    "assert 'email' in df.columns and 'summary' in df.columns, \"JSON must contain 'email' and 'summary' columns.\"\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_summaries, val_summaries = train_test_split(\n",
    "    df['email'], df['summary'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display data statistics\n",
    "print(f\"Training size: {len(train_texts)}\")\n",
    "print(f\"Validation size: {len(val_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailSummaryDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        summary = self.summaries.iloc[idx]\n",
    "\n",
    "        # Tokenize input and output\n",
    "        inputs = self.tokenizer(\n",
    "            text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            summary, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels[\"input_ids\"].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for T5-small distilled model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = EmailSummaryDataset(train_texts, train_summaries, tokenizer)\n",
    "val_dataset = EmailSummaryDataset(val_texts, val_summaries, tokenizer)\n",
    "\n",
    "# DataLoader parameters\n",
    "batch_size = 4  # Increased batch size for better GPU utilization\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5-small distilled model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training configuration\n",
    "epochs = 1\n",
    "log_interval = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|████████████████████████████████████| 24239/24239 [16:22:56<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", ncols=100)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to fine_tuned_summary_t5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "commented out so the model does not overwrite \n",
    "\"\"\"\n",
    "\n",
    "# # Directory to save the model\n",
    "# output_dir = \"fine_tuned_summary_t5\"\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Save model and tokenizer\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# print(f\"Model and tokenizer saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Michael is available for coffee on the first day of the conference, at 3 pm, near the conference venue. He offers to meet for coffee on the first day of the conference and offers a café near the venue.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model for testing\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"saved_t5_summary_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"saved_t5_summary_model\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Generate a summary for a test email\n",
    "test_email = \"I'm free for coffee on the first day of the conference, around 3 pm. There's a great little café near the conference venue.\"\n",
    "inputs = tokenizer(test_email, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=2, early_stopping=True)\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
