{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 96956\n",
      "Validation size: 24239\n"
     ]
    }
   ],
   "source": [
    "# load data from JSON file, with each object separated by a newline\n",
    "file_path = \"data/email_summaries_data.json\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# ensure the proper column summary and email exist \n",
    "assert 'email' in df.columns and 'summary' in df.columns, \"JSON must contain 'email' and 'summary' columns.\"\n",
    "\n",
    "# split the data into training and validation\n",
    "train_texts, val_texts, train_summaries, val_summaries = train_test_split(df['email'], df['summary'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Display data statistics\n",
    "print(f\"training size: {len(train_texts)}\")\n",
    "print(f\"validation size: {len(val_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailSummaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This prepares a dataset of email texts and the corresponding summaries to be fine tuned. \n",
    "    It tokenizes the data and ensures inputs and outputs are in the correct format for fine tuning. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: A pandas df of input emails\n",
    "            summaries: corresponding summaries for the input texts\n",
    "            tokenizer: the tokenizer associated with the T5 model to convert text to tokens\n",
    "            max_length: maximum token length for input and output sequences\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    # return length of input data\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # Retrieve a single data sample by its index, process it, and return it in tokenized form.\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        summary = self.summaries.iloc[idx]\n",
    "\n",
    "        # tokenize input and output\n",
    "        inputs = self.tokenizer(\n",
    "            text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            summary, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels[\"input_ids\"].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  T5-small tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# create the datasets for training and validation\n",
    "train_dataset = EmailSummaryDataset(train_texts, train_summaries, tokenizer)\n",
    "val_dataset = EmailSummaryDataset(val_texts, val_summaries, tokenizer)\n",
    "\n",
    "# set DataLoader parameters\n",
    "batch_size = 4  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the T5-small distilled model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# adjust model parameters for testing\n",
    "\n",
    "# Set optimizer to adam optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training configuration (hyperparameters)\n",
    "epochs = 1\n",
    "log_interval = 10\n",
    "\n",
    "# try decreasing learning rate for more reliable convergence\n",
    "# Optimizer: AdamW, Learning Rate: 1e-5, Epochs: 2\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# epochs = 2\n",
    "# log_interval = 10  # Log every 10 steps\n",
    "\n",
    "# try gradient clipping to prevent exploding gradients\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# epochs = 2\n",
    "# log_interval = 10\n",
    "# gradient_clip_val = 1.0  # Clip gradients to a max norm of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|████████████████████████████████████| 24239/24239 [16:22:56<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", ncols=100)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to fine_tuned_summary_t5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "commented out so the model does not overwrite \n",
    "\"\"\"\n",
    "\n",
    "# # Directory to save the model\n",
    "# output_dir = \"fine_tuned_summary_t5\"\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Save model and tokenizer\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# print(f\"Model and tokenizer saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Michael is available for coffee on the first day of the conference, at 3 pm, near the conference venue. He offers to meet for coffee on the first day of the conference and offers a café near the venue.\n"
     ]
    }
   ],
   "source": [
    "# load the fine-tuned model for testing\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"saved_t5_summary_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"saved_t5_summary_model\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# generate a summary for a test email\n",
    "test_email = \"I'm free for coffee on the first day of the conference, around 3 pm. There's a great little café near the conference venue.\"\n",
    "inputs = tokenizer(test_email, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=2, early_stopping=True)\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
