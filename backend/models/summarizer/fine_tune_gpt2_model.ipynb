{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune + train GPT2-distilled \n",
    "### the following code was run in a google colab for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/owendolan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 96956\n",
      "Validation samples: 24239\n"
     ]
    }
   ],
   "source": [
    "### USE THE FOLLOWING PATH IF ON COLAB\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "file_path = '/content/drive/MyDrive/email_summaries_data.json'\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\"\"\"\n",
    "\n",
    "### USE THE FOLLOWING IF RUNNING LOCALLY\n",
    "file_path = \"../../data/email_summaries_data.json\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# ensure json has the correct format\n",
    "assert 'email' in df.columns and 'summary' in df.columns, \"JSON must contain 'email' and 'summary' columns.\"\n",
    "\n",
    "# split the data into training and validation sets\n",
    "train_texts, val_texts, train_summaries, val_summaries = train_test_split(df['email'], df['summary'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailSummaryDataset(Dataset):\n",
    "\n",
    "    # initializes the dataset with texts, summaries, a tokenizer, and max token length\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    # returns the number of examples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # gets one example input text and summary at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        summary = self.summaries.iloc[idx]\n",
    "\n",
    "        # concatenate text and summary as a single sequence for GPT-2\n",
    "        combined = f\"Summarize: {text} Summary: {summary}\"\n",
    "\n",
    "        # tokenizes the summary similar to above with input\n",
    "        inputs = self.tokenizer(\n",
    "            combined,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # returns a dictionary containing input IDs, attention masks, and labels\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to train and evaluate model given hyperparameters \n",
    "def train_and_evaluate_model(learning_rate=5e-5, batch_size=4, max_length=256, epochs=1):\n",
    "\n",
    "    # load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "    \n",
    "    # set padding token and adjust padding side -- this fixes an issue with inputs being wrong size (not sure why this is)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"  # Correct padding for decoder-only models\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # resize the token embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # setup the datasets\n",
    "    train_dataset = EmailSummaryDataset(train_texts, train_summaries, tokenizer, max_length)\n",
    "    val_dataset = EmailSummaryDataset(val_texts, val_summaries, tokenizer, max_length)\n",
    "\n",
    "    # setup the data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # set optimizer to adam\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # start training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "\n",
    "        # perform a single training step calculate the loss and update the weights using Adam\n",
    "        for batch in tqdm(train_loader, desc=\"Training\", ncols=100):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # calculate outputs and loss \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # perform back propigation to update values\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # calculate training loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # run model evaluation loop\n",
    "    model.eval()\n",
    "\n",
    "    total_bleu = 0 # set bleu score\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\", ncols=100):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # generate summaries\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                max_new_tokens=50  \n",
    "            )\n",
    "            predicted_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # gets reference summary + calculates BLEU\n",
    "            reference_summary = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            bleu_score = sentence_bleu([reference_summary.split()], predicted_summary.split())\n",
    "            total_bleu += bleu_score\n",
    "\n",
    "    # print bleu score \n",
    "    avg_bleu_score = total_bleu / len(val_loader)\n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "\n",
    "    return avg_train_loss, avg_bleu_score # return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration 1:\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 4\n",
    "max_length = 256\n",
    "epochs = 1\n",
    "\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate, batch_size, max_length, epochs)\n",
    "print(f\"Configuration 1: Default Hyperparameters\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_and_evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# configuration 2:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_loss, bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration 2: Higher Learning Rate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_and_evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "# configuration 2:\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 4\n",
    "max_length = 256\n",
    "epochs = 1\n",
    "\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate, batch_size, max_length, epochs)\n",
    "print(f\"Configuration 2: Higher Learning Rate\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration 3: \n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 8\n",
    "max_length = 256\n",
    "epochs = 1\n",
    "\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate, batch_size, max_length, epochs)\n",
    "print(f\"Configuration 3: Larger Batch Size\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration 4:\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 4\n",
    "max_length = 512\n",
    "epochs = 1\n",
    "\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate=5e-5, batch_size=4, max_length=512, epochs=1)\n",
    "print(f\"Configuration 4: Longer Sequences\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./gpt2-finetuned-email-summary\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "commented out so the model does not overwrite \n",
    "\n",
    "The below code was used to save the particular model with the best performance to be called from the summarize_t5 output. \n",
    "\"\"\"\n",
    "\n",
    "# output_dir = \"./gpt2-finetuned-email-summary\"\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "# print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST: LOAD MODEL IN \n",
    "\"\"\"\n",
    "\n",
    "# fine-tuned model path\n",
    "model_path = \"./gpt2-finetuned-email-summary\"\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: we should eat yogurt, what do you think? I love sailing\n",
      "Generated Output: we should eat yogurt, what do you think? I love sailing. and.. to.\n"
     ]
    }
   ],
   "source": [
    "# test input\n",
    "test_input = \"we should eat yogurt, what do you think? I love sailing\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\") # tokenize\n",
    "\n",
    "# model output test\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,  # Maximum length of the generated text\n",
    "        num_beams=5,    # Beam search for better results\n",
    "        no_repeat_ngram_size=2,  # Avoid repetition\n",
    "        early_stopping=True      # Stop generating early if end is reached\n",
    "    )\n",
    "\n",
    "# decode and print the generated text\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Generated Output:\", output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
