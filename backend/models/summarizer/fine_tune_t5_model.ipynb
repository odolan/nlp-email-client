{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune + train T5-small \n",
    "### the following code was run in a google colab for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/owendolan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 96956\n",
      "validation: 24239\n"
     ]
    }
   ],
   "source": [
    "### USE THE FOLLOWING PATH IF ON COLAB\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "file_path = '/content/drive/MyDrive/email_summaries_data.json'\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\"\"\"\n",
    "\n",
    "### USE THE FOLLOWING IF RUNNING LOCALLY\n",
    "file_path = \"../../data/email_summaries_data.json\"\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "\n",
    "\n",
    "# ensure json has the correct format\n",
    "assert 'email' in df.columns and 'summary' in df.columns, \"JSON must contain 'email' and 'summary' columns.\"\n",
    "\n",
    "# split the data into training and validation sets\n",
    "train_texts, val_texts, train_summaries, val_summaries = train_test_split(df['email'], df['summary'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"training: {len(train_texts)}\")\n",
    "print(f\"validation: {len(val_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailSummaryDataset(Dataset):  \n",
    "\n",
    "    # initializes the dataset with texts, summaries, a tokenizer, and max token length\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=256): \n",
    "        self.texts = texts  \n",
    "        self.summaries = summaries \n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_length = max_length  \n",
    "\n",
    "    # returns the number of examples in the dataset\n",
    "    def __len__(self):  \n",
    "        return len(self.texts)\n",
    "\n",
    "    # gets one example input text and summary at the given index\n",
    "    def __getitem__(self, idx):  \n",
    "        text = self.texts.iloc[idx] \n",
    "        summary = self.summaries.iloc[idx]  \n",
    "\n",
    "        # tokenizes the input text, converts it to a tensor, and applies padding and truncation to fit the max length\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"  # returns tensors in PyTorch format\n",
    "        )\n",
    "\n",
    "        # tokenizes the summary similar to above with input\n",
    "        labels = self.tokenizer(\n",
    "            summary, \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"  # returns tensors in PyTorch format\n",
    "        )\n",
    "\n",
    "        # returns a dictionary containing input IDs, attention masks, and labels\n",
    "        return {  \n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(), \n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),  \n",
    "            \"labels\": labels[\"input_ids\"].squeeze()  \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to train and evaluate model given hyperparameters \n",
    "def train_and_evaluate_model(learning_rate=5e-5, batch_size=4, num_beams=2, epochs=1):\n",
    "   \n",
    "    # load tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # prepare datasets and dataloaders\n",
    "    train_dataset = EmailSummaryDataset(train_texts, train_summaries, tokenizer)\n",
    "    val_dataset = EmailSummaryDataset(val_texts, val_summaries, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # set optimizer to adam\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # start the training Loop\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        total_loss = 0 # track losss \n",
    "\n",
    "        ## ------ used GPT to help produce the below batch input \n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"Training\", ncols=100)):\n",
    "\n",
    "            # perform a single training step calculate the loss and update the weights using Adam\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # comput training loss \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # part 2, evaluation Loop\n",
    "    model.eval()\n",
    "    total_bleu = 0 # set inital bleu score\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\", ncols=100):\n",
    "\n",
    "            # produce one instancew of eval run\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # generate predictions\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, num_beams=num_beams, max_length=150, early_stopping=True)\n",
    "\n",
    "            # decode predictions and references\n",
    "            predicted_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            reference_summary = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "\n",
    "            # calculate bleu \n",
    "            bleu_score = sentence_bleu([reference_summary.split()], predicted_summary.split())\n",
    "            total_bleu += bleu_score\n",
    "\n",
    "    avg_bleu_score = total_bleu / len(val_loader)\n",
    "    print(f\"average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "\n",
    "    # return bleu and loss \n",
    "    return avg_train_loss, avg_bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration 1\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 4\n",
    "num_beams = 2\n",
    "\n",
    "# run training and evaluation\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate, batch_size, num_beams)\n",
    "\n",
    "print(f\"Configuration 1: Default Hyperparameters\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|████████████████████████████████████| 24239/24239 [16:22:56<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# configuration 2: \n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 4\n",
    "num_beams = 2\n",
    "\n",
    "# run training and evaluation\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate, batch_size, num_beams)\n",
    "\n",
    "print(f\"Configuration 2: Higher Learning Rate\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration 3:\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 8\n",
    "num_beams = 2\n",
    "\n",
    "# run training and evaluation\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate, batch_size, num_beams)\n",
    "\n",
    "print(f\"Configuration 3: Larger Batch Size\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration 4: \n",
    "\n",
    "learning_rate = 1e-5\n",
    "batch_size = 4\n",
    "num_beams = 4\n",
    "\n",
    "# run training and evaluation\n",
    "train_loss, bleu_score = train_and_evaluate_model(learning_rate, batch_size, num_beams)\n",
    "\n",
    "print(f\"Configuration 4: More Beams, Lower Learning Rate\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to fine_tuned_summary_t5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "commented out so the model does not overwrite \n",
    "\n",
    "The below code was used to save the particular model with the best performance to be called from the summarize_t5 output. \n",
    "\"\"\"\n",
    "\n",
    "# # Directory to save the model\n",
    "# output_dir = \"fine_tuned_summary_t5\"\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Save model and tokenizer\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# print(f\"Model and tokenizer saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Michael is available for coffee on the first day of the conference, at 3 pm, near the conference venue. He offers to meet for coffee on the first day of the conference and offers a café near the venue.\n"
     ]
    }
   ],
   "source": [
    "# load the fine-tuned model for testing\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"saved_t5_summary_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"saved_t5_summary_model\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# generate a summary for a test email\n",
    "test_email = \"I'm free for coffee on the first day of the conference, around 3 pm. There's a great little café near the conference venue.\"\n",
    "inputs = tokenizer(test_email, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=2, early_stopping=True)\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
